<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <link href="http://arxiv.org/api/query?search_query%3Dautogen%26id_list%3D%26start%3D0%26max_results%3D3" rel="self" type="application/atom+xml"/>
    <title type="html">ArXiv Query: search_query=autogen&amp;id_list=&amp;start=0&amp;max_results=3</title>
    <id>http://arxiv.org/api/FluLrO1hvaPrW3rTn9wZisgcIzQ</id>
    <updated>2024-10-16T00:00:00-04:00</updated>
    <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">21</opensearch:totalResults>
    <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
    <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">3</opensearch:itemsPerPage>
    <entry>
        <id>http://arxiv.org/abs/2410.03524v1</id>
        <updated>2024-10-04T15:44:47Z</updated>
        <published>2024-10-04T15:44:47Z</published>
        <title>Steering Large Language Models between Code Execution and Textual
  Reasoning</title>
        <summary>  While a lot of recent research focuses on enhancing the textual reasoning
capabilities of Large Language Models (LLMs) by optimizing the multi-agent
framework or reasoning chains, several benchmark tasks can be solved with 100%
success through direct coding, which is more scalable and avoids the
computational overhead associated with textual iterating and searching. Textual
reasoning has inherent limitations in solving tasks with challenges in math,
logics, optimization, and searching, which is unlikely to be solved by simply
scaling up the model and data size. The recently released OpenAI GPT Code
Interpreter and multi-agent frameworks such as AutoGen have demonstrated
remarkable proficiency of integrating code generation and execution to solve
complex tasks using LLMs. However, based on our experiments on 7 existing
popular methods for steering code/text generation in both single- and
multi-turn settings with 14 tasks and 6 types of LLMs (including the new
O1-preview), currently there is no optimal method to correctly steer LLMs to
write code when needed. We discover some interesting patterns on when models
use code vs. textual reasoning with the evolution to task complexity and model
sizes, which even result in an astonishingly inverse scaling law. We also
discover that results from LLM written code are not always better than using
textual reasoning, even if the task could be solved through code. To mitigate
the above issues, we propose three methods to better steer LLM code/text
generation and achieve a notable improvement. The costs of token lengths and
runtime are thoroughly discussed for all the methods. We believe the problem of
steering LLM code/text generation is critical for future research and has much
space for further improvement. Project Page, Datasets, and Codes are available
at https://yongchao98.github.io/CodeSteer/.
</summary>
        <author>
            <name>Yongchao Chen</name>
        </author>
        <author>
            <name>Harsh Jhamtani</name>
        </author>
        <author>
            <name>Srinagesh Sharma</name>
        </author>
        <author>
            <name>Chuchu Fan</name>
        </author>
        <author>
            <name>Chi Wang</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 12 figures, 12 tables</arxiv:comment>
        <link href="http://arxiv.org/abs/2410.03524v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/2410.03524v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/2408.15411v1</id>
        <updated>2024-08-27T21:21:13Z</updated>
        <published>2024-08-27T21:21:13Z</published>
        <title>AUTOGENICS: Automated Generation of Context-Aware Inline Comments for
  Code Snippets on Programming Q&amp;A Sites Using LLM</title>
        <summary>  Inline comments in the source code facilitate easy comprehension,
reusability, and enhanced readability. However, code snippets in answers on Q&amp;A
sites like Stack Overflow (SO) often lack comments because answerers volunteer
their time and often skip comments or explanations due to time constraints.
Existing studies show that these online code examples are difficult to read and
understand, making it difficult for developers (especially novices) to use them
correctly and leading to misuse. Given these challenges, we introduced
AUTOGENICS, a tool designed to integrate with SO to generate effective inline
comments for code snippets in SO answers exploiting large language models
(LLMs). Our contributions are threefold. First, we randomly select 400 answer
code snippets from SO and generate inline comments for them using LLMs. We then
manually evaluate these comments' effectiveness using four key metrics:
accuracy, adequacy, conciseness, and usefulness. Overall, LLMs demonstrate
promising effectiveness in generating inline comments for SO answer code
snippets. Second, we surveyed 14 active SO users to perceive the effectiveness
of these inline comments. The survey results are consistent with our previous
manual evaluation. However, according to our evaluation, LLMs-generated
comments are less effective for shorter code snippets and sometimes produce
noisy comments. Third, to address the gaps, we introduced AUTOGENICS, which
extracts additional context from question texts and generates context-aware
inline comments. It also optimizes comments by removing noise (e.g., comments
in import statements and variable declarations). We evaluate the effectiveness
of AUTOGENICS-generated comments using the same four metrics that outperform
those of standard LLMs. AUTOGENICS might (a) enhance code comprehension, (b)
save time, and improve developers' ability to learn and reuse code more
accurately.
</summary>
        <author>
            <name>Suborno Deb Bappon</name>
        </author>
        <author>
            <name>Saikat Mondal</name>
        </author>
        <author>
            <name>Banani Roy</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation in the research track at the IEEE
  International Conference on Source Code Analysis &amp; Manipulation (SCAM 2025)</arxiv:comment>
        <link href="http://arxiv.org/abs/2408.15411v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/2408.15411v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/2408.13406v1</id>
        <updated>2024-08-23T23:11:08Z</updated>
        <published>2024-08-23T23:11:08Z</published>
        <title>Optimizing Collaboration of LLM based Agents for Finite Element Analysis</title>
        <summary>  This paper investigates the interactions between multiple agents within Large
Language Models (LLMs) in the context of programming and coding tasks. We
utilize the AutoGen framework to facilitate communication among agents,
evaluating different configurations based on the success rates from 40 random
runs for each setup. The study focuses on developing a flexible automation
framework for applying the Finite Element Method (FEM) to solve linear elastic
problems. Our findings emphasize the importance of optimizing agent roles and
clearly defining their responsibilities, rather than merely increasing the
number of agents. Effective collaboration among agents is shown to be crucial
for addressing general FEM challenges. This research demonstrates the potential
of LLM multi-agent systems to enhance computational automation in simulation
methodologies, paving the way for future advancements in engineering and
artificial intelligence.
</summary>
        <author>
            <name>Chuan Tian</name>
        </author>
        <author>
            <name>Yilei Zhang</name>
        </author>
        <link href="http://arxiv.org/abs/2408.13406v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/2408.13406v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
</feed>
